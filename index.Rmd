---
title: "Jump Bikes in Sacramento"
author: "Tyler Jackson, Ryan Miller, Mitchell Snyder"
date: "2/22/2019"
output:
  html_document:
    theme: journal
    code_folding: show
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidycensus)
options(tigris_use_cache = TRUE)
library(tidyverse)
library(mapview)
library(sf)
library(scales)
library(r2d3maps)
library(ggmap)
library(lubridate)
library(ggthemes)
census_api_key("9c132628caf60947e2e0a7457b8744d69bd1eee0", overwrite = TRUE, install = TRUE) #for tinycensus
register_google(key = "AIzaSyCiRtI91wGVQH7Yn2dynoJulxVO2RxgALk")
```

```{r}
#read in the data

nov_jump <- read.csv("./data/sac_rides_nov.csv")

str(nov_jump)
# the given data


head(nov_jump[, 12:13]) #12,13 14,15


#select start and end coords into dfs to use later
start_coord <- nov_jump[,13:12]
end_coord <- nov_jump[,15:14]

colnames(start_coord) <- c("lon", "lat")
colnames(end_coord) <- c("lon", "lat")


# create linestring sf to map (we can also do this in ggplot without an sf)
# https://stackoverflow.com/questions/20531066/convert-begin-and-end-coordinates-into-spatial-lines-in-r


# Create list of simple feature geometries (linestrings)
l_sf <- vector("list", nrow(nov_jump))
for (i in seq_along(l_sf)){
  l_sf[[i]] <- st_linestring(as.matrix(rbind(start_coord[i, ], end_coord[i, ])))
}
# Create simple feature geometry list column
l_sfc <- st_sfc(l_sf, crs = "+proj=longlat +datum=WGS84")




#create sf points for analysis
start_sf <- st_as_sf(x = start_coord[!is.na(start_coord$lat),], 
                        coords = c("lon", "lat"),
                        crs = "+proj=longlat +datum=WGS84")

end_sf <- st_as_sf(x = end_coord[!is.na(end_coord$lat),], 
                        coords = c("lon", "lat"),
                        crs = "+proj=longlat +datum=WGS84")

#the sf doesn't have more than just geometry so I put the lat/long back in to use as a join column
start_sf$lat <- start_coord$lat
start_sf$lon <- start_coord$lon
#mapview(start_sf, col.regions = "red") + end_sf

```


#Load Census Data
```{r cache = TRUE}
# don't mess with this chunk unless we're adding more census data
#v16 <- load_variables(2017, "acs5", cache = TRUE)
#View(v16)
#get acs data for ca 
ca_tracts <- get_acs(geography = "tract", 
              year = 2017,
              variables = c(nhwhite = "B03002_003", 
                            nhblk = "B03002_004", 
                            nhasn = "B03002_006", 
                            nhisp = "B03002_012",
                            medincome = "B19013_001", 
                            totpop = "B01003_001",
                            totpov = "B17001_002",
                            incpercap ="B19301_001"), 
              state = "CA",
              survey = "acs5",
              geometry = TRUE,
              cache_table = TRUE)

names(ca_tracts)

#tidy up ca.tracts
ca_tracts <- ca_tracts %>%
  select(GEOID, NAME, variable, estimate, geometry) %>%
  spread(key = variable, value = estimate)

#create pop percentrage variables
ca_tracts <- mutate(ca_tracts, 
       pwht = nhwhite/totpop, 
       pasn   = nhasn/totpop, 
       pblk   = nhblk/totpop, 
       phisp    = nhisp/totpop, 
       ppov     = totpov/totpop)

#filter by county
jump_tracts <- ca_tracts %>%
  filter(str_detect(NAME, "Yolo|Sacramento*"))

#now set this chunk to cache and don't touch!
```

# exploring maps
## let's go on a date
```{r}
#make a day/time variable for lubridate to do some graphing
nov_jump$start_dt <- mdy_hms(paste(nov_jump$date_start,
                                   nov_jump$time_start), 
                             tz = "America/Los_Angeles", 
                             locale = Sys.getlocale("LC_TIME"))

#differentiate weekend from weekday rides
nov_jump$start_day_type <- ifelse(wday(nov_jump$start_dt, 
                                       week_start = 1) > 5,
                                  "Weekend", "Weekday")
nov_jump$mm <- hour(nov_jump$start_dt)*60 + 
  minute(nov_jump$start_dt)

nov_jump$week <- as.factor(week(nov_jump$start_dt))


nov_jump$start_day <- wday(nov_jump$start_dt, 
                           label = TRUE, 
                           abbr = FALSE, 
                           week_start = 1)
```

## play with plots
```{r}
#view start times by weekend/weekday
ggplot(nov_jump, aes(x= mm, fill = week)) + 
  geom_density(alpha=.6) +
  scale_x_continuous(labels = c("5am","8am","12:30pm","5pm","8pm"),
                     breaks = c(300,480,750,1020,1200)) + 
  labs(fill="",title="Jump Bike Nov 2018 Start Times") + 
  theme_fivethirtyeight() +
  theme(strip.background = element_rect(fill = "#FFFFFF")) +
  facet_grid(vars(start_day_type)) + 
  scale_fill_viridis_d(option="A")


#get freq per week
by_week <- nov_jump %>%
  group_by(week) %>%
  summarise(rides = length(eventType))

#plot it
ggplot(by_week) +
  geom_point(aes(x = week, y = rides))

#get freq per day for each week

nov_jump %>%
  group_by(week) %>%
  filter(week == "44") %>%
  select("start_day") %>% 
  group_by(start_day) %>%
  summarise(rides = length(week))

nov_jump %>%
  group_by(week) %>%
  filter(week == "45") %>%
  select("start_day") %>% 
  group_by(start_day) %>%
  summarise(rides = length(week))

nov_jump %>%
  group_by(week) %>%
  filter(week == "46") %>%
  select("start_day") %>% 
  group_by(start_day) %>%
  summarise(rides = length(week))

# we are missing days of data the 8th and the 14th :(

#check projection for the start_sf and the jump_tracts
st_crs(start_sf)

st_crs(jump_tracts)

#make them the same for joining
jump_tracts <- st_transform(jump_tracts, crs = 4326)

#join together (spatial join)
start_sf <- st_join(start_sf, jump_tracts, 
                    suffix = c(".start", ".start"), left = TRUE)

#merge tract census data with jump bike data
nov_jump <- merge(nov_jump, start_sf, 
                  by.x = "start_long", 
                  by.y = "lon", 
                  suffix = c(".start", ".start"), 
                  all.x = TRUE)

#this code gets a pretty map for using ggmap
#sac <- get_map(location = c(left= min(start_coord$lon), 
 #                           bottom = min(start_coord$lat), 
  #                          right = max(start_coord$lon), 
   #                         top = max(start_coord$lat)),
    #           source = "stamen",
     #          maptype = "terrain",
      #         zoom = 13)

#get ride counts in tracts
ride_in_tract <- st_join(start_sf, jump_tracts, join = st_within)

ride_count <- count(as_tibble(ride_in_tract), GEOID.x) %>%
  rename(GEOID = GEOID.x, start_count = n) %>%
  print()

#merge counts
jump_tracts <- left_join(jump_tracts, ride_count)

``` 

## D3

```{r eval=FALSE, include=FALSE}

d3_map(shape = jump_tracts, stroke_col = "#585858") %>%
  add_tooltip(value = "{NAME}: {n}") %>% 
  add_continuous_gradient2(low = muted("blue"), 
                           high = muted("red"), 
                           var = "n", range = c(0, 5500)) %>% 
  add_legend(title = "Poverty Rate by Tract") %>% 
  add_labs(title = "Yolo and Sacramento Counties",
    caption = "Source: ACS 2017 5 Year Estimates")
```

# Do jump bike origins and destinations happen in socio-economic enclaves in Sacramento?

Need origin / point data and census data for sacramento: need to decide which SES variable from census -> attach to point data

## Autocorrelation

Are there clusters in the Jump bike use? Are they located in enclaves?

## Origin / Destination point access buffers

Who has access to jump bikes? 

## Spatial modelling

GWR

# Do jump bike origins / destinations tend towards specific types of land use / zoning?

## Buffers / Zoning

Need to bring in zoning data and understand the zoning, how do we assess the neighbors?

## Modeling? does it make sense ie do zoning types influence jump bike ridership?

# Does weather / timing impact Jump bike ridership?

## Similar to the airplane data

compare ridership to Wind, air quality, temperature, percipitation

## is correlation? Modeling?


#Paper Guidelines
Your final paper should be much the same format as a peer-reviewed journal paper. Typically final papers run 15-18 pages including tables and figures (single-spaced). If you can get everything across in 10 solid pages, that's great. The general format of the final product should follow this outline:

* Brief introduction and overview of the literature you have reviewed. I do not expect you to have a deep knowledge of the literature, but you should have identified some driving questions that emerge from the literature. State your objectives/hypotheses clearly at the end of the introduction/lit review
* Methods. Give a clear and detailed summary of your data and methods

* Results. Describe your results. Make sure sure you label all tables and figures.

* Conclusion and discussion. Provide a summary of what you have learned about your research question along with the EDA/statistical results that support your conclusions. Also critique your own methods and provide suggestions for improving your analysis. Issues pertaining to the reliability and validity of your data, and appropriateness of the analysis approach should be discussed here. A paragraph on what you would do differently if you were able to start over with the project or what you would do next if you were going to continue work on the project should also be included.

