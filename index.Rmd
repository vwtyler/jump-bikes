---
title: "Jump Bikes in Sacramento"
author: "Tyler Jackson, Ryan Miller, Mitchell Snyder"
date: "2/22/2019"
output:
  html_document:
    theme: journal
    code_folding: show
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidycensus)
options(tigris_use_cache = TRUE)
library(tidyverse)
library(mapview)
library(sf)
library(scales)
#library(r2d3maps)
library(ggmap)
library(lubridate)
library(ggthemes)
library(cartogram)
library(tmap)
library(magrittr)
library(data.table)
library(jtools)
library(kableExtra)
```

```{r}
#read in the data

nov_jump <- read.csv("./data/sac_rides_nov.csv")

str(nov_jump)
# the given data


head(nov_jump[, 12:13]) #12,13 14,15


#select start and end coords into dfs to use later
start_coord <- nov_jump[,13:12]
end_coord <- nov_jump[,15:14]

colnames(start_coord) <- c("lon", "lat")
colnames(end_coord) <- c("lon", "lat")


# create linestring sf to map (we can also do this in ggplot without an sf)
# https://stackoverflow.com/questions/20531066/convert-begin-and-end-coordinates-into-spatial-lines-in-r


# Create list of simple feature geometries (linestrings)
l_sf <- vector("list", nrow(nov_jump))
for (i in seq_along(l_sf)){
  l_sf[[i]] <- st_linestring(as.matrix(rbind(start_coord[i, ], end_coord[i, ])))
}
# Create simple feature geometry list column
l_sfc <- st_sfc(l_sf, crs = "+proj=longlat +datum=WGS84")




#create sf points for analysis
start_sf <- st_as_sf(x = start_coord[!is.na(start_coord$lat),], 
                        coords = c("lon", "lat"),
                        crs = "+proj=longlat +datum=WGS84")

end_sf <- st_as_sf(x = end_coord[!is.na(end_coord$lat),], 
                        coords = c("lon", "lat"),
                        crs = "+proj=longlat +datum=WGS84")

#the sf doesn't have more than just geometry so I put the lat/long back in to use as a join column
start_sf$lat <- start_coord$lat
start_sf$lon <- start_coord$lon

end_sf$lat <- end_coord$lat
end_sf$lon <- end_coord$lon
#mapview(start_sf, col.regions = "red") + end_sf

```


#Load Census Data
```{r cache = TRUE}
# don't mess with this chunk unless we're adding more census data
#v16 <- load_variables(2017, "acs5", cache = TRUE)
#View(v16)
#get acs data for ca 
ca_tracts <- get_acs(geography = "tract", 
              year = 2017,
              variables = c(nhwhite = "B03002_003", 
                            nhblk = "B03002_004", 
                            nhasn = "B03002_006", 
                            nhisp = "B03002_012",
                            medincome = "B19013_001", 
                            totpop = "B01003_001",
                            totpov = "B17001_002",
                            incpercap ="B19301_001"), 
              state = "CA",
              survey = "acs5",
              geometry = TRUE,
              cache_table = TRUE)

names(ca_tracts)

#tidy up ca.tracts
ca_tracts <- ca_tracts %>%
  select(GEOID, NAME, variable, estimate, geometry) %>%
  spread(key = variable, value = estimate)

#create pop percentrage variables
ca_tracts <- mutate(ca_tracts, 
       pwht = nhwhite/totpop, 
       pasn   = nhasn/totpop, 
       pblk   = nhblk/totpop, 
       phisp    = nhisp/totpop, 
       ppov     = totpov/totpop)

#filter by county
jump_tracts <- ca_tracts %>%
  filter(str_detect(NAME, "Yolo|Sacramento*"))

#now set this chunk to cache and don't touch!
```

# exploring maps
## let's go on a date
```{r}
#make a day/time variable for lubridate to do some graphing
nov_jump$start_dt <- mdy_hms(paste(nov_jump$date_start,
                                   nov_jump$time_start), 
                             tz = "America/Los_Angeles", 
                             locale = Sys.getlocale("LC_TIME"))

#differentiate weekend from weekday rides
nov_jump$start_day_type <- ifelse(wday(nov_jump$start_dt) > 5,
                                  "Weekend", "Weekday")
nov_jump$mm <- hour(nov_jump$start_dt)*60 + 
  minute(nov_jump$start_dt)

nov_jump$week <- as.factor(week(nov_jump$start_dt))


nov_jump$start_day <- wday(nov_jump$start_dt)
```

## play with plots
```{r}
#view start times by weekend/weekday
ggplot(nov_jump, aes(x= mm, fill = week)) + 
  geom_density(alpha=.6) +
  scale_x_continuous(labels = c("5am","8am","12:30pm","5pm","8pm"),
                     breaks = c(300,480,750,1020,1200)) + 
  labs(fill="", title="Jump Bike Nov 2018 Start Times", 
       subtitle = "by Week") + 
  theme_fivethirtyeight() +
  theme(strip.background = element_rect(fill = "#FFFFFF")) +
  facet_grid(vars(start_day_type)) + 
  scale_fill_viridis_d(option="A")


#get freq per week
by_week <- nov_jump %>%
  group_by(week) %>%
  summarise(rides = length(eventType))

#plot it
ggplot(by_week) +
  geom_point(aes(x = week, y = rides))

#get freq per day for each week

nov_jump %>%
  group_by(week) %>%
  filter(week == "44") %>%
  select("start_day") %>% 
  group_by(start_day) %>%
  summarise(rides = length(week))

nov_jump %>%
  group_by(week) %>%
  filter(week == "45") %>%
  select("start_day") %>% 
  group_by(start_day) %>%
  summarise(rides = length(week))

nov_jump %>%
  group_by(week) %>%
  filter(week == "46") %>%
  select("start_day") %>% 
  group_by(start_day) %>%
  summarise(rides = length(week))

# we are missing days of data the 8th and the 14th :(

#check projection for the start_sf and the jump_tracts
st_crs(start_sf)

st_crs(jump_tracts)

#make them the same for joining
jump_tracts <- st_transform(jump_tracts, crs = 4326)

#join together (spatial join)
start_sf <- st_join(start_sf, jump_tracts, 
                    suffix = c(".start", ".start"), left = TRUE)

#merge tract census data with jump bike data
nov_jump <- merge(nov_jump, start_sf, 
                  by.x = "start_long", 
                  by.y = "lon", 
                  suffix = c(".start", ".start"), 
                  all.x = TRUE)

#this code gets a pretty map for using ggmap
#sac <- get_map(location = c(left= min(start_coord$lon), 
 #                           bottom = min(start_coord$lat), 
  #                          right = max(start_coord$lon), 
   #                         top = max(start_coord$lat)),
    #           source = "stamen",
     #          maptype = "terrain",
      #         zoom = 13)

#get start ride counts in tracts
jump_tracts <- st_transform(jump_tracts, crs = 6418)
start_sf <- st_transform(start_sf, 6418)
ride_in_tract <- st_join(start_sf, jump_tracts, join = st_within)

ride_count <- count(as_tibble(ride_in_tract), GEOID.x) %>%
  rename(GEOID = GEOID.x, start_count = n) %>%
  print()

jump_tracts <- left_join(jump_tracts, ride_count)

#get end ride counts in tracts
end_sf <- st_transform(end_sf, 6418)
ride_in_tract <- st_join(end_sf, jump_tracts, join = st_within)

ride_count <- count(as_tibble(ride_in_tract), GEOID) %>%
  rename(GEOID = GEOID, end_count = n) %>%
  print()

#merge counts
jump_tracts <- left_join(jump_tracts, ride_count)

jump_tracts <- jump_tracts %>%
  mutate(tot_rides = start_count + end_count)

```

```{r} 
ggplot(jump_tracts) + 
  geom_point(aes(ppov, tot_rides))

jump_tracts <- st_transform(jump_tracts, crs = 6418)
jump_cart <- cartogram_ncont(jump_tracts, "ppov")

#tm_shape(jump_cart) + tm_polygons("totpov")

``` 


#land use data
```{r}
#get land use data 
unzip(zipfile = "./data/Parcels.zip", exdir = "./data")

sac_parcels <- st_read("./data/Parcels.shp")

head(sac_parcels)

levels(sac_parcels$LU_GENERAL)
st_crs(jump_tracts)

#set proper crs
sac_parcels <- st_transform(sac_parcels, 6418)
sac_parcels_int <- st_intersection(sac_parcels, jump_tracts)


sac_parcels_agg <- aggregate(LU_SPECIF ~ GEOID + LU_GENERAL,
                             sac_parcels_int, FUN = length)
sac_parcels_agg <- sac_parcels_agg %>% 
  spread(key = LU_GENERAL, value = LU_SPECIF) %>%
  replace(is.na(.), 0)

sac_parcels_agg$sumpar <- rowSums(sac_parcels_agg[,-1])

jump_tracts <- left_join(jump_tracts, sac_parcels_agg)

jump_tracts <- jump_tracts %>%
  mutate(resRate = Residential/sumpar, retRate = `Retail / Commercial`/sumpar, riderate = tot_rides/totpop, offrate = Office/sumpar)
names(jump_tracts)
#is it neccesary to do this?
#sapply(sac_parcels$LU_GENERAL, switch, "Agriculture" = 1, "Care / Health" = 2, "Church / Welfare" = 3, "Industrial" = 4, "Miscellaneous" = 5, "Office" = 6, "Public / Utilities" = 7, "Recreational" = 8, "Residential" = 9, "Retail / Commercial" = 10, "Vacant" = 11, USE.NAMES = F)
```

```{r}
ggplot(jump_tracts) +
  geom_point(aes(retRate, start_count))

ggplot(jump_tracts) +
  geom_point(aes(resRate, start_count))
```

## D3

```{r eval=FALSE, include=FALSE}
jump_tracts <- st_transform(jump_tracts, 4326)
d3_map(shape = jump_tracts, stroke_col = "#585858") %>%
  add_tooltip(value = "{NAME}: {riderate}") %>% 
  add_continuous_gradient2(low = muted("blue"), 
                           high = muted("red"), 
                           var = "riderate", range = c(0, 5)) %>% 
  add_legend(title = "Poverty Rate by Tract") %>% 
  add_labs(title = "Yolo and Sacramento Counties",
    caption = "Source: ACS 2017 5 Year Estimates")
```

```{r}
names(jump_tracts)

  
pairs(riderate ~ offrate + resRate + retRate + ppov + pwht, jump_tracts)

jump_lm <- lm(riderate ~ offrate + resRate + retRate + ppov + pwht + pasn + pblk + phisp, jump_tracts)
summary(jump_lm)

summ(jump_lm)

par(mfrow=c(2,2))
plot(jump_lm)
```

```{r}
mapview(jump_tracts, zcol = "ppov")
```

Make a heatmat?
```{r eval=FALSE, include=FALSE}

heatmap.start_end <- list()
heatmap.start_end$start <- start_sf %>% 
  group_by(lon, lat) %>%
  summarize(intensity = sqrt(n()))
names(heatmap.start_end$start)[1:2] <- c("longitude","latitude")
 
heatmap.start_end$end <- end_sf %>%
  group_by(lon, lat) %>%
  summarize(intensity = sqrt(n()))
names(heatmap.start_end$end)[1:2] <- c("longitude","latitude")
 
heatmap.start_end$start$pos <- "Start"
heatmap.start_end$end$pos <- "End"
 
heatmap.start_end %<>% rbindlist(fill = T)

```

```{r eval=FALSE, include=FALSE}
library(leaflet)
library(leaflet.extras)
leaflet(heatmap.start_end) %>% 
  addProviderTiles(providers$CartoDB) %>%
  addHeatmap(data = heatmap.start_end %>% filter(pos=="Start"),
             lng=~longitude, 
             lat=~latitude, 
             intensity = ~intensity,
             blur = 10, 
             max = 100, radius = 15,
             layerId = "Start", group = "Start") %>%
  addHeatmap(data = heatmap.start_end %>% filter(pos=="End"),
             lng=~longitude, 
             lat=~latitude, 
             intensity = ~intensity,
             blur = 10, 
             max = 100, radius = 15,
             layerId = "End", group = "End") %>%
  addLayersControl(
    baseGroups = c("Start","End"),
    options = layersControlOptions(collapsed = FALSE)
  )
```


```{r}
tmap_mode("view")
tm_shape(jump_tracts) + tm_polygons("ppov")

tmap_mode("view")
tm_shape(jump_tracts) + tm_polygons("retRate")

tmap_mode("view")
tm_shape(jump_tracts) + tm_polygons("riderate")
```

# Do jump bike origins and destinations happen in socio-economic enclaves in Sacramento? 
 
Need origin / point data and census data for sacramento: need to decide which SES variable from census -> attach to point data

## Autocorrelation

Are there clusters in the Jump bike use? Are they located in enclaves?

## Origin / Destination point access buffers

Who has access to jump bikes? 

## Spatial modelling

GWR

# Do jump bike origins / destinations tend towards specific types of land use / zoning?

## Buffers / Zoning

Need to bring in zoning data and understand the zoning, how do we assess the neighbors?

## Modeling? does it make sense ie do zoning types influence jump bike ridership?

# Does weather / timing impact Jump bike ridership?

## Similar to the airplane data

compare ridership to Wind, air quality, temperature, percipitation

## is correlation? Modeling?


#Paper Guidelines
Your final paper should be much the same format as a peer-reviewed journal paper. Typically final papers run 15-18 pages including tables and figures (single-spaced). If you can get everything across in 10 solid pages, that's great. The general format of the final product should follow this outline:

* Brief introduction and overview of the literature you have reviewed. I do not expect you to have a deep knowledge of the literature, but you should have identified some driving questions that emerge from the literature. State your objectives/hypotheses clearly at the end of the introduction/lit review
* Methods. Give a clear and detailed summary of your data and methods
```{r}

```

* Results. Describe your results. Make sure sure you label all tables and figures.

These are our results, these are our plotted residuals:
```{r echo=FALSE}
par(mfrow=c(2,2))
plot(jump_lm)
```


* Conclusion and discussion. Provide a summary of what you have learned about your research question along with the EDA/statistical results that support your conclusions. 

* Also critique your own methods and provide suggestions for improving your analysis. Issues pertaining to the reliability and validity of your data, and appropriateness of the analysis approach should be discussed here. 

* A paragraph on what you would do differently if you were able to start over with the project or what you would do next if you were going to continue work on the project should also be included.

#Paper

Bike Sharing Systems (BSS) have become increasingly viable options for transportation in the built environment. BSS provide bicycles to consumers across urban areas (typically in hubs called “stations”) on an as-needed basis, allowing commuters and leisure-seekers to ride for a small fee without the downsides (initial cost, maintenance, storage, etc,) of bike ownership (Rixey, 2013; Smith et al. 2015). In addition to environmental and health benefits of BSS, many programs market these systems to connect “first and last mile” commuters, connecting people to transit hubs, work, or city centers (Smith et al. 2015). A relatively recent *newcomer* to the United States, Bike Sharing Systems have grown rapidly since 2012, due in part to technology constraints of bicycles and urban infrastructure built to accommodate heavy bicycle use. Numerous technological and logistical improvements to BSS have helped current “fourth-generation” services like JUMP bikes to grow in popularity among urban populations (Smith et al. 2015).
As the number and quality of Bike Sharing Services have increased over time, so too have concerns of inequity linked to these programs (McNeil et al 2018). Put simply, the economic and logistical prerogative to maximize profit and centralize BSS hubs has resulted in the clustering of services in and around city centers—centers that cater to a predominately white, middle- or upper-class user. While acknowledging the complex and varied role of politics, race, and class play in shaping the built environment, the scope of this project will determine whether BSS usage is linked to socio-economic enclaves. 

To that end, this project closely analyzed Goodman and Handy’s 2015 report “Providing Equitable Access to Sacramento’s Bike Share System” to gain a critical framework for our data analysis. Tellingly, Goodman and Handy find that “Contemporary ridership forecasting models for bike share are based on current patterns of bicycle use, and generally assume a negative correlation between ridership and prevalence of non-white population, though these rates may be due to other factors such as poor infrastructure or blight rather than the LIM [Low Income Minority] residents themselves.” (Goodman and Handy, 2015; brackets added). The timing of the report (2015) prior to the implementation of the JUMP bikes in May of 2018 make our data set from November 2018 a good predictor of user trends, allowing for a six month adjustment/learning period. Whereas Goodman and Handy set out to “explore how system equity barriers could be removed to that all residents … could enjoy the benefits of bike share” this report sets out to analyze the correlation between socio-economic status (ses) and JUMP bike ridership to see if matches with industry trends (Goodman and Handy, 2015). Our null hypothesis is that JUMP bike users in Sacramento WILL/ WILL NOT conform to the general trend that favors users from higher socio-economic enclaves


Paper Guidelines
Your final paper should be much the same format as a peer-reviewed journal paper. Typically final papers run 15-18 pages including tables and figures (single-spaced). If you can get everything across in 10 solid pages, that's great. The general format of the final product should follow this outline:
* Brief introduction and overview of the literature you have reviewed. I do not expect you to have a deep knowledge of the literature, but you should have identified some driving questions that emerge from the literature. State your objectives/hypotheses clearly at the end of the introduction/lit review

* Methods. Give a clear and detailed summary of your data and methods
•	Got census data (2017 AM COM SURV) and jump bike data (nov 2018) and	 Land use data from Sac county (2018ish?)
•	Merged datasets together
•	Stats of Jmp bike data for trends by time of day and week day weekend use
•	Saw patterns that implicated commuter use
•	 Use trip locations (start and end) as proxy for the user, layered in neighborhood census data to code
•	Built mult regression model to test ridership against numerous variables (including hypothesis for LIM folks)
* Results. Describe your results. Make sure you label all tables and figures.
•	On separate part (with Tyler) Figures
o	Graph across time
o	Correlation table
o	Maps of variables 
	Ride rate
	Retail rate
	Poverty/ses
o	Heat map 
o	Regression output
o	Plot of residuals
•	Result of our regression shows that there is no link between LIM status and ridership (within the 2 week period)

* Conclusion and discussion. Provide a summary of what you have learned about your research question along with the Exploratory Data Analysis/statistical results that support your conclusions. 
•	Understood temporality
•	Suggested we could talk about commute patterns 
o	Learned using tract demograpics as a proxy for ridership does not establish a correlation between rdrship and demographic variables
Also critique your own methods and provide suggestions for improving your analysis. Issues pertaining to the reliability and validity of your data, and appropriateness of the analysis approach should be discussed here.
•	More time on dependent variables for nghbrhd characteristics
•	More stat testing more than OLS regression
o	Plus gwr/ running tests for multicollinearity heteroscedasticity 
 A paragraph on what you would do differently if you were able to start over with the project or what you would do next if you were going to continue work on the project should also be included.
We need a ridership level survey
•	GET MOER DATA.
•	Conduct ridership survey (cant make the overlying assumptions that we did in this class) 
•	Work with a data magician (statistician) 



REFERENCES:

Goodman, B and Handy, S. 2015. Providing Equitable Access to Sacramento’s Bike Share System. U.C. Davis Institute of Transportation Studies.

Rixey, A. 2013. Station-Level Forecasting of Bike Sharing Ridership: Station Network Effects in Three U.S. Systems. TRB 2013 Annual Meeting.

Smith, C.S., Oh, J., Lei, C. 2015. Exploring the Equity Dimensions of U.S. Bicycle Sharing Systems. Transportation Research Center for Livable Communities. 

United States Census Bureau (2010). 2010 California Census Data. Retrieved from: http://quickfacts.census.gov/qfd/states/06/0664000.html


